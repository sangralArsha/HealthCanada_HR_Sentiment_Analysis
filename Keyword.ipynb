{"cells":[{"cell_type":"code","execution_count":null,"id":"b04a998e-508f-49c6-839a-355aab5c3ac8","metadata":{"id":"b04a998e-508f-49c6-839a-355aab5c3ac8","outputId":"554ab0fd-5f79-4e78-df27-d0e920f21956"},"outputs":[{"name":"stdout","output_type":"stream","text":["  QUESTION                                            TITLE_E  \\\n","0      Q01  I have the tools, technology and equipment I n...   \n","1      Q02  The material and tools provided for my work, i...   \n","2      Q03  My physical environment (e.g., office, workspa...   \n","3      Q04            I get the training I need to do my job.   \n","4      Q05  I have the information, training and equipment...   \n","\n","                                            Keywords  \n","0          [tools, technology, need, job, equipment]  \n","1        [tools, work, software, provided, official]  \n","2  [workspace, suitable, requirements, physical, ...  \n","3                              [training, need, job]  \n","4        [work, training, safety, need, information]  \n"]}],"source":["import pandas as pd\n","from sklearn.feature_extraction.text import CountVectorizer\n","import numpy as np\n","\n","# Function to extract keywords from a text\n","def extract_keywords(texts, n_keywords=5):\n","    vectorizer = CountVectorizer(max_features=1000, stop_words='english')\n","    X = vectorizer.fit_transform(texts)\n","    word_sum = np.array(X.sum(axis=0)).flatten()\n","    words = np.array(vectorizer.get_feature_names_out())\n","    sorted_idx = np.argsort(word_sum)[::-1]\n","    return [\", \".join(words[sorted_idx][:n_keywords])]\n","\n","# Load the Excel file\n","\n","df = pd.read_excel('QvsK.xlsx')\n","\n","# Apply the function to extract keywords for each question\n","df['Keywords'] = df['TITLE_E'].apply(lambda x: extract_keywords([x]))\n","\n","# Show the updated DataFrame\n","print(df.head())\n"]},{"cell_type":"code","execution_count":null,"id":"bfd6c5ef-af70-402e-b8e9-5fd60b142eca","metadata":{"id":"bfd6c5ef-af70-402e-b8e9-5fd60b142eca"},"outputs":[],"source":["df.to_excel('2nd_output_keys.xlsx')"]},{"cell_type":"code","execution_count":null,"id":"a294a0ce-9ad1-43ff-81f9-6c98dc637251","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":203},"id":"a294a0ce-9ad1-43ff-81f9-6c98dc637251","executionInfo":{"status":"error","timestamp":1710377717931,"user_tz":240,"elapsed":628,"user":{"displayName":"ndegwa ndumia","userId":"12950685544219444683"}},"outputId":"1904ddf9-e8e3-4479-9640-32eddf01feff"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'tokens' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-1930bdf18412>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'tokens' is not defined"]}],"source":["import pandas as pd\n","import spacy\n","from gensim import corpora, models\n","\n","# Data\n","data = [['tools  technology need job equipment'],\n","        ['tools work software provided official'],\n","        ['workspace suitable requirements physical office'],\n","        ['training need job'],\n","        ['work training safety need information'],\n","        ['work support personal life balance']]\n","\n","df = pd.DataFrame(data, columns=['Keyword'])\n","# Load spaCy English model (download if not installed)\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","def preprocess_text(text):\n","  # Lowercase, remove stopwords, and lemmatize\n","  doc = nlp(text.lower())\n","  tokens = [token.lemma_ for token in doc if token.is_alpha]  # Keep only alphabetic tokens\n","  return \" \".join(tokens)\n","\n","\n","\n","# Preprocess each row\n","df['Processed_Text'] = df.apply(lambda row: \" \".join(row.iloc[:5]), axis=1)\n","df['Processed_Text'] = df['Processed_Text'].apply(preprocess_text)\n","# Create dictionary (mapping words to unique IDs)\n","dictionary = corpora.Dictionary(df['Processed_Text'])\n","\n","# Create DTM\n","dtm = [dictionary.doc2bow(text) for text in df['Processed_Text']]\n","# Train LDA model (adjust num_topics as needed)\n","lda_model = models.LdaModel(dtm, id2word=dictionary, num_topics=3)\n","\n","# Get topics and top words for each topic\n","topics = lda_model.show_topics(formatted=False)\n","\n","# Assign topic to each row based on the dominant topic\n","df['Topic'] = df['Processed_Text'].apply(lambda text: lda_model[dictionary.doc2bow(text)][0][1]).astype(str)  # Extract dominant topic ID and convert to string\n","\n","print(df)"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.0"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}